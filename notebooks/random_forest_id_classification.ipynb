{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Proyecto de Clasificación de IDs Válidos\n",
    "\n",
    "Este notebook tiene como objetivo clasificar IDs en válidos o no válidos utilizando un modelo de Random Forest. Los datos se extraen de una base de datos Teradata y se procesan en lotes debido al gran tamaño del conjunto de datos.\n"
   ],
   "id": "285103f05eca490c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuración del Entorno\n",
    "\n",
    "Primero, importamos las librerías necesarias para la ejecución del notebook:"
   ],
   "id": "61f912075841497b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T21:41:27.142172Z",
     "start_time": "2024-08-20T21:41:27.080209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "aa3d3ce4387be761",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T21:41:31.206595Z",
     "start_time": "2024-08-20T21:41:27.445592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src import check_ids, get_connection, close_connection\n",
    "from teradataml import execute_sql\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import math\n",
    "from teradataml.dataframe.fastload import fastload"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuración Inicial\n",
    "\n",
    "Definimos los parámetros y rutas necesarios para el procesamiento de datos:\n",
    "\n",
    "- **`batch_size`**: Tamaño del lote para el procesamiento de datos.\n",
    "- **`query_file`**: Ruta al archivo SQL que contiene la consulta para extraer los datos.\n",
    "- **`columns`**: Lista de nombres de columnas esperadas en el DataFrame.\n"
   ],
   "id": "30123798884639b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:10:27.652129Z",
     "start_time": "2024-08-21T13:10:27.581091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 1000000\n",
    "query_file = '../sql/queries/customerid_inactive_2m.sql'\n",
    "columns = ['UNIDAD DE NEGOCIO', 'ZONA','REGION','CUSTOMER_ID', 'LLAVE','MIN FECHA','MAX FECHA','TRANSACTIONS','MONEY','FECHAS','VOLUME','QTY']"
   ],
   "id": "29dfa12ee69abcc7",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Establecimiento de Conexión\n",
    "\n",
    "Conectamos a la base de datos Teradata utilizando la función `get_connection()`, que se encarga de establecer la conexión y verificar que las credenciales estén correctas.\n"
   ],
   "id": "713329ffe6269aa1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:05:51.202090Z",
     "start_time": "2024-08-21T13:05:45.637650Z"
    }
   },
   "cell_type": "code",
   "source": "get_connection()",
   "id": "d7282b9444ca283e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión a Teradata Vantage establecida correctamente.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Carga del Modelo y Preprocesador\n",
    "\n",
    "Cargamos los modelos y preprocesadores necesarios para la clasificación de IDs. Utilizamos `joblib` para cargar el preprocesador y el modelo de Random Forest preentrenado desde archivos pickle (.pkl).\n"
   ],
   "id": "ac0796707dfa73e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:05:51.299223Z",
     "start_time": "2024-08-21T13:05:51.202090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessor = joblib.load('../models/preprocessor.pkl')\n",
    "model = joblib.load('../models/random_forest_model.pkl')"
   ],
   "id": "ee3d8918136aefc1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extracción de Datos en Lotes\n",
    "\n",
    "Procedemos a extraer y procesar los datos en lotes debido al gran tamaño del conjunto de datos. Calculamos el número total de registros y la cantidad de lotes necesarios, y luego ejecutamos la consulta SQL para cada lote.\n"
   ],
   "id": "36a87dc6475147d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:10:31.793081Z",
     "start_time": "2024-08-21T13:10:31.723585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_data = pd.DataFrame()\n",
    "\n",
    "with open(query_file, 'r') as file:\n",
    "    query_template = file.read()"
   ],
   "id": "e1fb79a7021a7d78",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T13:10:46.003426Z",
     "start_time": "2024-08-21T13:10:32.783115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_query = \"\"\"\n",
    "    select count(*) from col_mkt.info_per_customer C\n",
    "    left join \n",
    "    col_mkt.customer_predictions P\n",
    "    on C.UNIDAD_DE_NEGOCIO = P.UNIDAD_DE_NEGOCIO\n",
    "    AND C.ZONA = P.ZONA\n",
    "    AND C.REGION = P.REGION\n",
    "    AND C.CUSTOMER_ID = P.CUSTOMER_ID\n",
    "    WHERE P.CUSTOMER_ID IS NULL;\n",
    "    \"\"\"\n",
    "\n",
    "total_records = pd.DataFrame(execute_sql(count_query))[0][0]\n",
    "total_batches = math.ceil(total_records / batch_size)\n",
    "print(total_batches)"
   ],
   "id": "bf49a2cfdf8b3261",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Procesamiento de Datos y Predicción\n",
    "\n",
    "Para cada lote de datos:\n",
    "\n",
    "1. **Conversión de Datos**: Convertimos las columnas a los formatos necesarios.\n",
    "2. **Aplicación del Modelo**: Usamos el modelo para predecir la validez de los IDs.\n",
    "3. **Carga de Resultados**: Insertamos los resultados en una tabla auxiliar en Teradata y luego en la tabla final.\n",
    "4. **Manejo de Errores**: Capturamos y mostramos errores en caso de problemas durante la ejecución."
   ],
   "id": "f7d02988fbe644ae"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-21T13:11:05.277031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch in range(total_batches):\n",
    "    print(batch)\n",
    "    start = batch * batch_size + 1\n",
    "    end = start + batch_size - 1\n",
    "    query = query_template.format(start=start, end=end)\n",
    "    try:\n",
    "        \n",
    "        result = execute_sql(query)\n",
    "        print('Consulta hecha correctamente.')\n",
    "        batch_df = pd.DataFrame(result, columns=columns)\n",
    "        batch_df['VOLUME'] = pd.to_numeric(batch_df['VOLUME'], errors='coerce')\n",
    "        batch_df['QTY'] = pd.to_numeric(batch_df['QTY'], errors='coerce')\n",
    "        batch_df['MONEY'] = pd.to_numeric(batch_df['MONEY'], errors='coerce')\n",
    "        batch_df['MIN FECHA'] = pd.to_datetime(batch_df['MIN FECHA'], format='%Y-%m-%d')\n",
    "        batch_df['MAX FECHA'] = pd.to_datetime(batch_df['MAX FECHA'], format='%Y-%m-%d')\n",
    "        batch_df[['ID_LENGTH', 'ID_HAS_REPEATED_DIGITS', 'ID_IS_ASCENDING', 'ID_IS_DESCENDING', 'ID_HAS_REPETITIVE_PATTERN']] = batch_df[\n",
    "            'CUSTOMER_ID'].apply(lambda x: pd.Series(check_ids(x)))\n",
    "        \n",
    "        input_processed = preprocessor.transform(batch_df)\n",
    "        prediction = model.predict(input_processed)\n",
    "        \n",
    "        batch_df['IS_VALID'] = prediction\n",
    "        batch_df = batch_df[['UNIDAD DE NEGOCIO','ZONA','REGION','CUSTOMER_ID','IS_VALID']]\n",
    "        \n",
    "        print('Insertando en Teradata...')\n",
    "        \n",
    "        fastload(df=batch_df, schema_name='COL_MKT', table_name='CUSTOMER_PREDICTIONS_AUX',if_exists='replace')\n",
    "        \n",
    "        print('Inserción auxiliar hecha')\n",
    "        \n",
    "        insert_query_predictions = \"\"\"\n",
    "            INSERT INTO COL_MKT.CUSTOMER_PREDICTIONS\n",
    "            SELECT * FROM COL_MKT.CUSTOMER_PREDICTIONS_AUX;\n",
    "            \"\"\"\n",
    "        \n",
    "        \n",
    "        execute_sql(insert_query_predictions)\n",
    "        \n",
    "        print(f'Inserción hecha de {batch_df.shape[0]}')\n",
    "        \n",
    "        all_data = pd.concat([all_data, batch_df], ignore_index=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error al ejecutar la consulta para el lote {batch}: {e}')"
   ],
   "id": "faf6d4f815ffd367",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Consulta hecha correctamente.\n",
      "Insertando en Teradata...\n",
      "Processed 108857 rows in batch 1.\n",
      "Processed 108857 rows in batch 2.\n",
      "Processed 108857 rows in batch 3.\n",
      "Processed 108857 rows in batch 4.\n",
      "Processed 108857 rows in batch 5.\n",
      "Processed 108857 rows in batch 6.\n",
      "Processed 108857 rows in batch 7.\n",
      "Processed 217711 rows in batch 8.\n",
      "Inserción auxiliar hecha\n",
      "Inserción hecha de 979710\n",
      "1\n",
      "Consulta hecha correctamente.\n",
      "Insertando en Teradata...\n",
      "Processed 109591 rows in batch 1.\n",
      "Processed 109591 rows in batch 2.\n",
      "Processed 109591 rows in batch 3.\n",
      "Processed 109591 rows in batch 4.\n",
      "Processed 109591 rows in batch 5.\n",
      "Processed 109591 rows in batch 6.\n",
      "Processed 109591 rows in batch 7.\n",
      "Processed 219179 rows in batch 8.\n",
      "Inserción auxiliar hecha\n",
      "Inserción hecha de 986316\n",
      "2\n",
      "Consulta hecha correctamente.\n",
      "Insertando en Teradata...\n",
      "Processed 107401 rows in batch 1.\n",
      "Processed 107401 rows in batch 2.\n",
      "Processed 107401 rows in batch 3.\n",
      "Processed 107401 rows in batch 4.\n",
      "Processed 107401 rows in batch 5.\n",
      "Processed 107401 rows in batch 6.\n",
      "Processed 107401 rows in batch 7.\n",
      "Processed 214798 rows in batch 8.\n",
      "Inserción auxiliar hecha\n",
      "Inserción hecha de 966605\n",
      "3\n",
      "Consulta hecha correctamente.\n",
      "Insertando en Teradata...\n",
      "Processed 108058 rows in batch 1.\n",
      "Processed 108058 rows in batch 2.\n",
      "Processed 108058 rows in batch 3.\n",
      "Processed 108058 rows in batch 4.\n",
      "Processed 108058 rows in batch 5.\n",
      "Processed 108058 rows in batch 6.\n",
      "Processed 108058 rows in batch 7.\n",
      "Processed 216112 rows in batch 8.\n",
      "Inserción auxiliar hecha\n",
      "Inserción hecha de 972518\n",
      "4\n",
      "Consulta hecha correctamente.\n",
      "Insertando en Teradata...\n",
      "Processed 100735 rows in batch 1.\n",
      "Processed 100735 rows in batch 2.\n",
      "Processed 100735 rows in batch 3.\n",
      "Processed 100735 rows in batch 4.\n",
      "Processed 100735 rows in batch 5.\n",
      "Processed 100735 rows in batch 6.\n",
      "Processed 100735 rows in batch 7.\n",
      "Processed 100735 rows in batch 8.\n",
      "Processed 201468 rows in batch 9.\n",
      "Inserción auxiliar hecha\n",
      "Inserción hecha de 1007348\n",
      "5\n",
      "Consulta hecha correctamente.\n",
      "Insertando en Teradata...\n",
      "Processed 104531 rows in batch 1.\n",
      "Processed 104531 rows in batch 2.\n",
      "Processed 104531 rows in batch 3.\n",
      "Processed 104531 rows in batch 4.\n",
      "Processed 104531 rows in batch 5.\n",
      "Processed 104531 rows in batch 6.\n",
      "Processed 104531 rows in batch 7.\n",
      "Processed 104531 rows in batch 8.\n",
      "Processed 209061 rows in batch 9.\n",
      "Inserción auxiliar hecha\n",
      "Inserción hecha de 1045309\n",
      "6\n",
      "Consulta hecha correctamente.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cierre de Conexión\n",
    "\n",
    "Al finalizar el procesamiento de todos los lotes, cerramos la conexión a la base de datos utilizando la función `close_connection()`. Esto asegura que no queden conexiones abiertas y libera recursos."
   ],
   "id": "3f5560057fe2c457"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T23:25:12.600613Z",
     "start_time": "2024-08-20T23:25:11.068288Z"
    }
   },
   "cell_type": "code",
   "source": "close_connection()",
   "id": "29cdd340cfeb096",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión cerrada correctamente.\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
